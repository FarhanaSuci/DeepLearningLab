{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a036ac-4ca4-4968-8fc9-87900f15ef63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[0.48767498 0.39013998 0.46816798 0.46654053]\n",
      " [0.51332912 0.51334077 0.51252074 0.51334208]\n",
      " [0.54036009 0.54035317 0.54035824 0.54036009]\n",
      " [0.56880009 0.56880009 0.56880009 0.56880009]\n",
      " [0.59873694 0.59873694 0.59873694 0.59873694]\n",
      " [0.63024941 0.63024941 0.63024941 0.63024941]\n",
      " [0.66342043 0.66342043 0.66342043 0.66342043]\n",
      " [0.6983373  0.6983373  0.6983373  0.6983373 ]\n",
      " [0.73509189 0.73509189 0.73509189 0.73509189]\n",
      " [0.77378094 0.77378094 0.77378094 0.77378094]\n",
      " [0.81450625 0.81450625 0.81450625 0.81450625]\n",
      " [0.857375   0.857375   0.857375   0.857375  ]\n",
      " [0.9025     0.9025     0.9025     0.9025    ]\n",
      " [0.95       0.95       0.95       0.95      ]\n",
      " [1.         1.         1.         1.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "n_states = 16  # Number of states in the grid world\n",
    "n_actions = 4  # Number of possible actions (up, down, left, right)\n",
    "goal_state = 15  # Goal state\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Define parameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "# Q-learning algorithm\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0, n_states)  # Start from a random state\n",
    "\n",
    "    while current_state != goal_state:\n",
    "        # Choose action with epsilon-greedy strategy\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = np.random.randint(0, n_actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])  # Exploit\n",
    "\n",
    "        # Simulate the environment (move to the next state)\n",
    "        # For simplicity, move to the next state\n",
    "        next_state = (current_state + 1) % n_states\n",
    "\n",
    "        # Define a simple reward function (1 if the goal state is reached, 0 otherwise)\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "\n",
    "        # Update Q-value using the Q-learning update rule\n",
    "        Q_table[current_state, action] += learning_rate * \\\n",
    "            (reward + discount_factor *\n",
    "             np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "\n",
    "        current_state = next_state  # Move to the next state\n",
    "\n",
    "# After training, the Q-table represents the learned Q-values\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e971d-5666-413d-a92c-6b182aa8334b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
